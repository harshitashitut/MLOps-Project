Gradient descent minimizes a loss function by updating parameters in the opposite direction of its gradient.
Update rule:

ğ‘¤ changes such that  wâˆ’Î±âˆ‡L(w) 

Where Î± is the learning rate.
